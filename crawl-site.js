#!/usr/bin/env node
/**
 * Site Crawler
 * Background crawler script that crawls a site and triggers classification on completion.
 * Uses Playwright for full JavaScript rendering to capture dynamic content.
 * 
 * Usage:
 *   node crawl-site.js --site-id=123 --limit=200 --exclude=/blog/page/*,/tag/*
 */
import { createClient } from '@supabase/supabase-js'
import { load } from 'cheerio'
import { chromium } from 'playwright'
import { spawn } from 'child_process'
import path from 'path'
import { fileURLToPath } from 'url'

const __dirname = path.dirname(fileURLToPath(import.meta.url))

// Lazy-load dotenv only when running as CLI
const isMainModule = process.argv[1]?.endsWith('crawl-site.js')
if (isMainModule) {
    await import('dotenv/config')
}

// Initialize Supabase lazily
let _supabase = null
function getSupabase() {
    if (!_supabase) {
        // Support both VITE_ prefixed (dev) and non-prefixed (Railway) env vars
        const supabaseUrl = process.env.SUPABASE_URL || process.env.VITE_SUPABASE_URL
        const supabaseKey = process.env.SUPABASE_SERVICE_KEY || process.env.SUPABASE_ANON_KEY ||
            process.env.VITE_SUPABASE_SERVICE_KEY || process.env.VITE_SUPABASE_ANON_KEY

        if (!supabaseUrl || !supabaseKey) {
            console.error('Supabase Init Failed. Env vars present:')
            console.error('SUPABASE_URL:', !!process.env.SUPABASE_URL)
            console.error('VITE_SUPABASE_URL:', !!process.env.VITE_SUPABASE_URL)
            console.error('SUPABASE_SERVICE_KEY:', !!process.env.SUPABASE_SERVICE_KEY)
            console.error('VITE_SUPABASE_SERVICE_KEY:', !!process.env.VITE_SUPABASE_SERVICE_KEY)
            throw new Error(`Missing Supabase credentials. URL: ${!!supabaseUrl}, Key: ${!!supabaseKey}`)
        }

        _supabase = createClient(supabaseUrl, supabaseKey)
    }
    return _supabase
}

const DELAY_MS = 500   // Delay between requests (slower for Playwright)
const TIMEOUT_MS = 30000  // Per-page timeout (30 sec for JS rendering)
const MAX_RETRIES = 2  // Retry failed pages up to 2 times
// No HTML length limit - capture full page content

// ============================================================
// Browser Lifecycle (Playwright)
// ============================================================

let _browser = null
let _context = null

async function getBrowser() {
    if (!_browser) {
        console.log('üåê Launching headless browser...')
        _browser = await chromium.launch({
            headless: true,
            args: ['--no-sandbox', '--disable-setuid-sandbox']
        })
        _context = await _browser.newContext({
            userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport: { width: 1280, height: 720 }
        })
        console.log('‚úì Browser ready')
    }
    return { browser: _browser, context: _context }
}

async function closeBrowser() {
    if (_browser) {
        await _browser.close()
        _browser = null
        _context = null
        console.log('üåê Browser closed')
    }
}

// ============================================================
// HTML Cleaning (from clean-html.js)
// ============================================================

const KEEP_ATTRIBUTES = new Set([
    'href', 'src', 'alt', 'title',
    'datetime', 'role', 'aria-label', 'aria-labelledby',
    'type', 'rel', 'name', 'content', 'property'
])

const REMOVE_TAGS = new Set([
    'script', 'style', 'noscript', 'iframe', 'svg', 'path', 'link', 'meta'
])

function cleanHtml(rawHtml) {
    if (!rawHtml) return ''

    const $ = load(rawHtml)

    // First, remove everything that's definitely not content
    REMOVE_TAGS.forEach(tag => $(tag).remove())

    // Remove navigation and non-content elements
    $('nav, header, footer, aside, noscript, iframe, form, [role="navigation"], [role="banner"], [role="contentinfo"], .nav, .navigation, .header, .footer, .sidebar').remove()

    // Remove images with base64 data URIs (they bloat the content)
    $('img').each((_, el) => {
        const src = $(el).attr('src') || ''
        if (src.startsWith('data:')) {
            $(el).remove()
        }
    })

    // Try to find main content area first
    let mainContent = $('main, article, [role="main"], .content, #content, .main-content, #main').first()

    // If no main content found, use body
    if (!mainContent.length) {
        mainContent = $('body')
    }

    // Clone and work with just the main content
    const $content = load(mainContent.html() || '')

    // Remove comments
    $content('*').contents().filter(function () {
        return this.type === 'comment'
    }).remove()

    // Remove remaining base64 images in the content area too
    $content('img').each((_, el) => {
        const src = $content(el).attr('src') || ''
        if (src.startsWith('data:')) {
            $content(el).remove()
        }
    })

    // Strip most attributes but keep structural ones
    $content('*').each((_, el) => {
        const $el = $content(el)
        const attribs = el.attribs || {}
        Object.keys(attribs).forEach(attr => {
            if (!KEEP_ATTRIBUTES.has(attr)) {
                $el.removeAttr(attr)
            }
        })
    })

    let cleaned = $content.html()
    if (!cleaned) return ''

    // Normalize whitespace while preserving structure for block elements
    // Add newlines after block-level closing tags to maintain visual structure
    const blockTags = ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'ul', 'ol', 'section', 'article', 'figure', 'figcaption', 'blockquote']
    blockTags.forEach(tag => {
        const regex = new RegExp(`</${tag}>`, 'gi')
        cleaned = cleaned.replace(regex, `</${tag}>\n`)
    })

    // Collapse multiple spaces/newlines into single space (but keep the newlines we added)
    cleaned = cleaned
        .replace(/[ \t]+/g, ' ')           // Collapse horizontal whitespace
        .replace(/\n\s*\n/g, '\n')         // Collapse multiple newlines
        .replace(/>\s+</g, '>\n<')         // Ensure tags have newlines between them
        .replace(/^\s+|\s+$/gm, '')        // Trim each line
        .trim()

    // No truncation - capture full page content for accurate analysis

    return cleaned
}

// ============================================================
// Page Parsing
// ============================================================

function parsePage(html, baseUrl) {
    const $ = load(html)
    const base = new URL(baseUrl)

    // Title
    const title = $('title').first().text().trim() || null

    // Meta description
    const metaDesc = $('meta[name="description"]').attr('content') || null

    // Canonical URL
    const canonicalUrl = $('link[rel="canonical"]').attr('href') || null

    // Extract internal and external links BEFORE removing elements
    const internalLinks = []
    const externalLinks = []
    $('a[href]').each((_, el) => {
        const href = $(el).attr('href')
        if (!href) return
        if (href.startsWith('mailto:') || href.startsWith('tel:') || href.startsWith('javascript:')) return
        // Skip malformed URLs containing HTML tags (e.g., broken iframe embeds)
        if (href.includes('<') || href.includes('%3C')) return

        try {
            const resolved = new URL(href, base.origin)
            // Remove fragment
            resolved.hash = ''
            if (resolved.hostname === base.hostname || resolved.hostname === base.hostname.replace(/^www\./, '') || base.hostname === resolved.hostname.replace(/^www\./, '')) {
                internalLinks.push(resolved.href)
            } else {
                externalLinks.push(resolved.href)
            }
        } catch {
            // Invalid URL, skip
        }
    })

    // Remove non-content elements BEFORE extracting headings and content
    $('script, style, nav, header, footer, aside, noscript, iframe, form, [role="navigation"], [role="banner"], [role="contentinfo"]').remove()

    // Now extract headings from cleaned content only
    const h1 = []
    const h2 = []
    const h3 = []
    $('h1').each((_, el) => {
        const text = $(el).text().trim()
        if (text && text.length > 2) h1.push(text)
    })
    $('h2').each((_, el) => {
        const text = $(el).text().trim()
        if (text && text.length > 2) h2.push(text)
    })
    $('h3').each((_, el) => {
        const text = $(el).text().trim()
        if (text && text.length > 2) h3.push(text)
    })

    // Extract structured content: headings and paragraphs in document order
    const structuredContent = []
    $('main, article, [role="main"], .content, #content, body').first()
        .find('h1, h2, h3, h4, p, li').each((_, el) => {
            const tagName = el.tagName.toLowerCase()
            const text = $(el).text().trim()
            if (!text || text.length < 3) return

            if (['h1', 'h2', 'h3', 'h4'].includes(tagName)) {
                structuredContent.push({ type: 'heading', level: tagName, text })
            } else if (tagName === 'p' || tagName === 'li') {
                // Truncate long paragraphs
                const truncated = text.length > 500 ? text.substring(0, 500) + '...' : text
                structuredContent.push({ type: 'paragraph', text: truncated })
            }
        })

    // Main content (full text extraction)
    const mainContent = $('main, article, [role="main"], .content, #content, body')
        .first().text().replace(/\s+/g, ' ').trim()

    return {
        title,
        meta_description: metaDesc,
        canonical_url: canonicalUrl,
        headings: { h1, h2, h3 },
        structured_content: structuredContent.slice(0, 100), // Limit to first 100 elements
        internal_links: [...new Set(internalLinks)],
        external_links: [...new Set(externalLinks)],
        main_content: mainContent
    }
}


// ============================================================
// URL Queue
// ============================================================

class UrlQueue {
    constructor(baseUrl, maxPages, excludePatterns = []) {
        this.baseUrl = new URL(baseUrl)
        this.maxPages = maxPages
        this.excludePatterns = excludePatterns
        this.queue = []
        this.seen = new Set()
        this.processedCount = 0
    }

    add(url, depth = 0) {
        const normalized = this.normalizeUrl(url)
        if (!normalized) return false
        if (this.seen.has(normalized)) return false
        if (!this.isSameDomain(normalized)) return false
        if (this.isExcluded(normalized)) return false

        this.queue.push({ url: normalized, depth })
        this.seen.add(normalized)
        return true
    }

    next() {
        return this.queue.shift() || null
    }

    markProcessed() {
        this.processedCount++
    }

    canContinue() {
        return this.processedCount < this.maxPages && this.queue.length > 0
    }

    normalizeUrl(url) {
        try {
            const parsed = new URL(url, this.baseUrl.origin)
            parsed.hash = ''
            parsed.searchParams.sort()
            // Remove trailing slash for consistency (except for root)
            let normalized = parsed.href
            if (normalized.endsWith('/') && parsed.pathname !== '/') {
                normalized = normalized.slice(0, -1)
            }
            return normalized
        } catch {
            return null
        }
    }

    isSameDomain(url) {
        try {
            const parsed = new URL(url)
            // Normalize by stripping www. prefix to handle redirects
            const normalizeHost = (host) => host.replace(/^www\./, '')
            return normalizeHost(parsed.hostname) === normalizeHost(this.baseUrl.hostname)
        } catch {
            return false
        }
    }

    isExcluded(url) {
        try {
            const parsed = new URL(url)
            const path = parsed.pathname

            for (const pattern of this.excludePatterns) {
                const regex = new RegExp('^' + pattern.replace(/\*/g, '.*') + '$')
                if (regex.test(path)) {
                    return true
                }
            }
            return false
        } catch {
            return false
        }
    }
}

// ============================================================
// Fetcher (Playwright)
// ============================================================

let lastFetchTime = 0

async function fetchPage(url, retryCount = 0) {
    // Rate limiting
    const elapsed = Date.now() - lastFetchTime
    if (elapsed < DELAY_MS) {
        await new Promise(resolve => setTimeout(resolve, DELAY_MS - elapsed))
    }

    let page = null
    try {
        const { context } = await getBrowser()
        page = await context.newPage()

        // Navigate and wait for network to be idle (JS loaded)
        const response = await page.goto(url, {
            timeout: TIMEOUT_MS,
            waitUntil: 'networkidle'  // Wait for all JS to load
        })

        lastFetchTime = Date.now()

        const statusCode = response?.status() || 0
        const contentType = response?.headers()['content-type'] || ''
        const isHtml = contentType.includes('text/html') || contentType.includes('xhtml')

        // Get the rendered HTML (after JS execution)
        const html = isHtml ? await page.content() : null
        const finalUrl = page.url()

        await page.close()

        return {
            url,
            finalUrl,
            statusCode,
            html,
            isHtml,
            contentType
        }
    } catch (error) {
        lastFetchTime = Date.now()

        if (page) {
            try { await page.close() } catch { }
        }

        // Retry on timeout or network errors
        if (retryCount < MAX_RETRIES && (error.message.includes('timeout') || error.message.includes('net::'))) {
            const backoffMs = Math.pow(2, retryCount) * 1000 // 1s, 2s, 4s...
            console.log(`   ‚è≥ Retry ${retryCount + 1}/${MAX_RETRIES} after ${backoffMs}ms...`)
            await new Promise(resolve => setTimeout(resolve, backoffMs))
            return fetchPage(url, retryCount + 1)
        }

        return {
            url,
            finalUrl: url,
            statusCode: 0,
            html: null,
            isHtml: false,
            error: error.message
        }
    }
}

// ============================================================
// Database Operations
// ============================================================

async function updateSiteStatus(siteId, status, currentUrl = null) {
    const updates = { crawl_status: status, updated_at: new Date().toISOString() }
    if (currentUrl) updates.current_url = currentUrl

    await getSupabase().from('site_index').update(updates).eq('id', siteId)
}

async function updatePagesCount(siteId, count) {
    await getSupabase().from('site_index')
        .update({ pages_crawled: count, updated_at: new Date().toISOString() })
        .eq('id', siteId)
}

async function savePage(siteId, url, data) {
    // Normalize URL: remove trailing slash (except root) for consistent upsert
    let normalizedUrl = url
    const parsed = new URL(url)
    if (normalizedUrl.endsWith('/') && parsed.pathname !== '/') {
        normalizedUrl = normalizedUrl.slice(0, -1)
    }
    const path = parsed.pathname.replace(/\/$/, '') || '/'

    const pageData = {
        site_id: siteId,
        url: normalizedUrl,
        path,
        title: data.title,
        status_code: data.statusCode,
        html_content: data.html,
        cleaned_html: data.cleanedHtml,
        main_content: data.main_content || null,
        headings: data.headings || null,
        meta_tags: { description: data.meta_description } || null,
        links_internal: data.internal_links || null,
        links_external: data.external_links || null,
        crawled_at: new Date().toISOString()
    }

    // Upsert to handle re-crawls
    const { error } = await getSupabase()
        .from('page_index')
        .upsert(pageData, { onConflict: 'site_id,url' })

    if (error) {
        console.error(`  ‚ùå Error saving ${path}:`, error.message)
    }
}

// ============================================================
// Classifier Integration
// ============================================================

// ============================================================
// Classifier Integration
// ============================================================

function executeClassifier(siteId) {
    return new Promise((resolve, reject) => {
        console.log(`\nüß† Starting page classification...`)

        // Import dynamically to avoid circular dependencies
        import('./classify-pages.js')
            .then(({ runClassification }) => {
                runClassification(siteId)
                    .then(resolve)
                    .catch(err => reject(new Error(`Classification failed: ${err.message}`)))
            })
            .catch(err => reject(new Error(`Failed to import classifier: ${err.message}`)))
    })
}

// ============================================================
// Main Crawler - Exported for in-process use
// ============================================================

export async function runCrawl(siteId, pageLimit = 200, excludePatterns = [], runClassifier = true) {
    if (!siteId) {
        throw new Error('siteId is required')
    }

    // Initialize queue with starting URL
    let queue;

    try {
        console.log(`\nüï∑Ô∏è Starting crawl for site ${siteId}`)
        console.log(`   Page limit: ${pageLimit}`)
        console.log(`   Exclude patterns: ${excludePatterns.length > 0 ? excludePatterns.join(', ') : 'none'}`)

        // Get site info
        const { data: site, error: siteError } = await getSupabase()
            .from('site_index')
            .select('*')
            .eq('id', siteId)
            .single()

        if (siteError || !site) {
            throw new Error(`Site not found: ${siteError?.message}`)
        }

        console.log(`   URL: ${site.url}`)

        queue = new UrlQueue(site.url, pageLimit, excludePatterns)
        queue.add(site.url)

        let pagesProcessed = 0
        await updateSiteStatus(siteId, 'in_progress')

        while (queue.canContinue()) {
            const item = queue.next()
            if (!item) break

            const { url } = item

            console.log(`\nüìÑ [${pagesProcessed + 1}/${pageLimit}] ${url}`)
            await updateSiteStatus(siteId, 'in_progress', url)

            // Fetch page
            const result = await fetchPage(url)

            if (!result.isHtml || !result.html) {
                console.log(`   ‚è≠Ô∏è Skipped (not HTML or error)`)
                continue
            }

            console.log(`   ‚úì Status: ${result.statusCode}`)

            // Parse page
            const parsed = parsePage(result.html, result.finalUrl)
            console.log(`   ‚úì Title: ${parsed.title?.slice(0, 50) || '(none)'}...`)

            // Check canonical - skip if it points elsewhere (e.g., paginated pages)
            if (parsed.canonical_url) {
                try {
                    const canonical = new URL(parsed.canonical_url, result.finalUrl).href
                    const current = result.finalUrl
                    // Normalize by removing trailing slashes for comparison
                    const normalizedCanonical = canonical.replace(/\/$/, '')
                    const normalizedCurrent = current.replace(/\/$/, '')
                    if (normalizedCanonical !== normalizedCurrent) {
                        console.log(`   ‚è≠Ô∏è Skipped (canonical points to ${parsed.canonical_url})`)
                        continue
                    }
                } catch {
                    // Invalid canonical URL, proceed anyway
                }
            }

            // Clean HTML
            const cleanedHtml = cleanHtml(result.html)
            console.log(`   ‚úì Cleaned: ${(result.html.length / 1024).toFixed(0)}KB ‚Üí ${(cleanedHtml.length / 1024).toFixed(0)}KB`)

            // Save to database
            await savePage(siteId, result.finalUrl, {
                ...parsed,
                statusCode: result.statusCode,
                html: result.html,
                cleanedHtml
            })

            // Add discovered links to queue
            let addedLinks = 0
            for (const link of parsed.internal_links) {
                if (queue.add(link, item.depth + 1)) {
                    addedLinks++
                }
            }
            console.log(`   ‚úì Discovered ${addedLinks} new links`)

            pagesProcessed++
            queue.markProcessed()
            await updatePagesCount(siteId, pagesProcessed)
        }

        console.log(`\n‚úÖ Crawl complete! Processed ${pagesProcessed} pages.`)

        // Run classifier (if enabled)
        if (runClassifier) {
            await updateSiteStatus(siteId, 'classifying')
            await executeClassifier(siteId)
        } else {
            console.log('‚è≠Ô∏è Classifier skipped (disabled by user)')
        }

        // Sync final page count from actual database count
        const { count } = await getSupabase()
            .from('page_index')
            .select('*', { count: 'exact', head: true })
            .eq('site_id', siteId)
        await updatePagesCount(siteId, count || pagesProcessed)

        // Mark complete
        await updateSiteStatus(siteId, 'complete')
        console.log(`\nüéâ Site ${siteId} fully processed!`)

    } catch (error) {
        console.error('\n‚ùå Crawl failed:', error.message)
        await updateSiteStatus(siteId, 'error')
        throw error
    } finally {
        // Always close browser when done
        await closeBrowser()
    }
}

// CLI entry point - only runs when executed directly
if (isMainModule) {
    const args = process.argv.slice(2)
    const siteIdArg = args.find(a => a.startsWith('--site-id='))
    const limitArg = args.find(a => a.startsWith('--limit='))
    const excludeArg = args.find(a => a.startsWith('--exclude='))

    const siteId = siteIdArg ? siteIdArg.split('=')[1] : null
    const pageLimit = limitArg ? parseInt(limitArg.split('=')[1], 10) : 200
    const excludePatterns = excludeArg ? excludeArg.split('=')[1].split(',') : []

    if (!siteId) {
        console.error('Usage: node crawl-site.js --site-id=123 --limit=200')
        process.exit(1)
    }

    runCrawl(siteId, pageLimit, excludePatterns)
        .then(() => process.exit(0))
        .catch(() => process.exit(1))
}

// Export functions for use by server.js
export { parsePage, cleanHtml }

// Standalone single-page fetch with Playwright (manages its own browser lifecycle)
export async function fetchPageWithPlaywright(url) {
    let browser = null
    let page = null

    try {
        console.log('üåê Launching browser for single-page fetch...')
        browser = await chromium.launch({
            headless: true,
            args: ['--no-sandbox', '--disable-setuid-sandbox']
        })
        const context = await browser.newContext({
            userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport: { width: 1280, height: 720 }
        })
        page = await context.newPage()

        // Navigate and wait for JS to load
        const response = await page.goto(url, {
            timeout: 30000,
            waitUntil: 'networkidle'
        })

        const statusCode = response?.status() || 0
        const contentType = response?.headers()['content-type'] || ''
        const isHtml = contentType.includes('text/html') || contentType.includes('xhtml')
        const html = isHtml ? await page.content() : null
        const finalUrl = page.url()

        console.log(`‚úì Fetched with Playwright: ${statusCode}`)

        return {
            url,
            finalUrl,
            statusCode,
            html,
            isHtml,
            contentType
        }
    } catch (error) {
        console.error('Playwright fetch error:', error.message)
        return {
            url,
            finalUrl: url,
            statusCode: 0,
            html: null,
            isHtml: false,
            error: error.message
        }
    } finally {
        if (browser) {
            await browser.close()
            console.log('üåê Browser closed')
        }
    }
}
